{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea024b30",
   "metadata": {},
   "source": [
    "Prebuilt Data From huggingface data hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8625a451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset,load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType,prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d55fcb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=load_dataset('roneneldan/TinyStories' , split='train')\n",
    "\n",
    "#loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "38a149ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2119719\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "372711a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\\n\\nOne day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Beep drove under the tree and watched the leaves fall on him. He laughed and beeped his horn.\\n\\nBeep played with the falling leaves all day. When it was time to go home, Beep knew he needed more fuel. He went to the fuel place and got more healthy fuel. Now, Beep was ready to go fast and play again the next day. And Beep lived happily ever after.'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3954b76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages extracted with fitz: 1\n"
     ]
    }
   ],
   "source": [
    "#using own pdf for domain specific finetuning\n",
    "#Steps\n",
    "#Data Collection\n",
    "# splitting or chunking\n",
    "# tokenization\n",
    "# training\n",
    "\n",
    "import fitz\n",
    "pages_text = []\n",
    "def extract_pdf_to_list_fitz(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "   \n",
    "\n",
    "    for page in doc:\n",
    "        text = page.get_text(\"text\").strip()\n",
    "        if text:\n",
    "            pages_text.append(text)\n",
    "    \n",
    "    doc.close()\n",
    "    return pages_text\n",
    "\n",
    "file_path = 'Metformin.pdf'\n",
    "pdf_content_list_fitz = extract_pdf_to_list_fitz(file_path)\n",
    "print(f\"Total pages extracted with fitz: {len(pdf_content_list_fitz)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c5c3cf45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Metformin is one of the most widely prescribed oral antihyperglycemic agents.\\u200b\\n Its primary mechanism of action involves the activation of AMP-activated protein kinase \\n(AMPK), a central metabolic regulator that promotes glucose uptake and fatty acid oxidation \\nwhile inhibiting hepatic gluconeogenesis.\\u200b\\n Beyond its glycemic control, Metformin has been shown to improve cardiovascular outcomes \\nand display anti-inflammatory properties.\\u200b\\n Recent studies also suggest potential anticancer effects through inhibition of the mTOR \\nsignaling pathway and suppression of tumor angiogenesis. \\n \\nClinical trials have demonstrated that combining Atorvastatin with Ezetimibe results in \\nsignificant reductions in low-density lipoprotein cholesterol (LDL-C) levels compared to \\nmonotherapy.\\u200b\\n Ezetimibe acts by inhibiting the Niemann–Pick C1-like 1 (NPC1L1) transporter in the intestinal \\nwall, reducing cholesterol absorption, while Atorvastatin inhibits hepatic HMG-CoA reductase, \\nsuppressing endogenous cholesterol synthesis.\\u200b\\n The dual mechanism provides an additive lipid-lowering effect, particularly beneficial for \\npatients with familial hypercholesterolemia who are unresponsive to statins alone. \\n \\nThe success of mRNA vaccines against SARS-CoV-2 has opened new pathways for rapid \\nvaccine development.\\u200b\\n mRNA platforms enable flexible design and quick adaptation to emerging viral variants such as \\nBQ.1 and XBB.1.5.\\u200b\\n Phase-II clinical trials have shown strong immunogenicity with elevated neutralizing antibody \\ntiters and robust CD8⁺ T-cell responses.\\u200b\\n Ongoing research is exploring thermostable formulations and self-amplifying mRNA constructs \\nto enhance global distribution and cost-efficiency. \\n \\nArtificial intelligence (AI) is transforming pharmaceutical research by accelerating target \\nidentification, molecular docking, and compound screening.\\u200b\\n Deep learning models trained on large-scale biological datasets can predict protein–ligand \\nbinding affinities and optimize lead compounds.\\u200b\\n Integrating AI-driven insights with laboratory automation is reducing discovery timelines from \\nyears to months.\\u200b\\n However, challenges remain regarding interpretability, bias mitigation, and regulatory validation \\nfor AI-generated molecules.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "028a9fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to chunks\n",
    "import re\n",
    "def chunk_paragraphs(txt):\n",
    "\n",
    "    paragraph = []\n",
    "    for block in txt:\n",
    "        chunks=re.split(r'\\n\\s*\\n', block)\n",
    "        for chunk in chunks:\n",
    "            clean=chunk.strip()\n",
    "            if len(clean)>30:\n",
    "                paragraph.append(clean)\n",
    "    return paragraph\n",
    "\n",
    "para=chunk_paragraphs(pages_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "98d1375b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Metformin is one of the most widely prescribed oral antihyperglycemic agents.\\u200b\\n Its primary mechanism of action involves the activation of AMP-activated protein kinase \\n(AMPK), a central metabolic regulator that promotes glucose uptake and fatty acid oxidation \\nwhile inhibiting hepatic gluconeogenesis.\\u200b\\n Beyond its glycemic control, Metformin has been shown to improve cardiovascular outcomes \\nand display anti-inflammatory properties.\\u200b\\n Recent studies also suggest potential anticancer effects through inhibition of the mTOR \\nsignaling pathway and suppression of tumor angiogenesis.',\n",
       " 'Clinical trials have demonstrated that combining Atorvastatin with Ezetimibe results in \\nsignificant reductions in low-density lipoprotein cholesterol (LDL-C) levels compared to \\nmonotherapy.\\u200b\\n Ezetimibe acts by inhibiting the Niemann–Pick C1-like 1 (NPC1L1) transporter in the intestinal \\nwall, reducing cholesterol absorption, while Atorvastatin inhibits hepatic HMG-CoA reductase, \\nsuppressing endogenous cholesterol synthesis.\\u200b\\n The dual mechanism provides an additive lipid-lowering effect, particularly beneficial for \\npatients with familial hypercholesterolemia who are unresponsive to statins alone.',\n",
       " 'The success of mRNA vaccines against SARS-CoV-2 has opened new pathways for rapid \\nvaccine development.\\u200b\\n mRNA platforms enable flexible design and quick adaptation to emerging viral variants such as \\nBQ.1 and XBB.1.5.\\u200b\\n Phase-II clinical trials have shown strong immunogenicity with elevated neutralizing antibody \\ntiters and robust CD8⁺ T-cell responses.\\u200b\\n Ongoing research is exploring thermostable formulations and self-amplifying mRNA constructs \\nto enhance global distribution and cost-efficiency.',\n",
       " 'Artificial intelligence (AI) is transforming pharmaceutical research by accelerating target \\nidentification, molecular docking, and compound screening.\\u200b\\n Deep learning models trained on large-scale biological datasets can predict protein–ligand \\nbinding affinities and optimize lead compounds.\\u200b\\n Integrating AI-driven insights with laboratory automation is reducing discovery timelines from \\nyears to months.\\u200b\\n However, challenges remain regarding interpretability, bias mitigation, and regulatory validation \\nfor AI-generated molecules.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1caa6032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertin to list\n",
    "data=[{\"text\":p} for p in para]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f7f09c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e2143050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Metformin is one of the most widely prescribed oral antihyperglycemic agents.\\u200b\\n Its primary mechanism of action involves the activation of AMP-activated protein kinase \\n(AMPK), a central metabolic regulator that promotes glucose uptake and fatty acid oxidation \\nwhile inhibiting hepatic gluconeogenesis.\\u200b\\n Beyond its glycemic control, Metformin has been shown to improve cardiovascular outcomes \\nand display anti-inflammatory properties.\\u200b\\n Recent studies also suggest potential anticancer effects through inhibition of the mTOR \\nsignaling pathway and suppression of tumor angiogenesis.'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aadbfdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the model\n",
    "\n",
    "base_model='TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d84febba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM,Trainer,TrainingArguments,DataCollatorForLanguageModeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "983e3ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(base_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "85b74a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token=tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9095b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization function\n",
    "def tokenize_fn(examples):\n",
    "    tokens=tokenizer(examples['text'],truncation=True,padding=\"max_length\",max_length=512)\n",
    "    tokens['labels']=tokens['input_ids'].copy()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "57c4a5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4/4 [00:00<00:00, 68.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized=dataset.map(tokenize_fn,batched=True,remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0c608aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1,\n",
       "  4737,\n",
       "  689,\n",
       "  262,\n",
       "  338,\n",
       "  697,\n",
       "  310,\n",
       "  278,\n",
       "  1556,\n",
       "  17644,\n",
       "  2225,\n",
       "  23059,\n",
       "  470,\n",
       "  284,\n",
       "  9418,\n",
       "  24947,\n",
       "  16808,\n",
       "  19335,\n",
       "  293,\n",
       "  19518,\n",
       "  29889,\n",
       "  30166,\n",
       "  13,\n",
       "  8011,\n",
       "  7601,\n",
       "  13336,\n",
       "  310,\n",
       "  3158,\n",
       "  20789,\n",
       "  278,\n",
       "  26229,\n",
       "  310,\n",
       "  319,\n",
       "  3580,\n",
       "  29899,\n",
       "  11236,\n",
       "  630,\n",
       "  26823,\n",
       "  19015,\n",
       "  559,\n",
       "  29871,\n",
       "  13,\n",
       "  29898,\n",
       "  19297,\n",
       "  29968,\n",
       "  511,\n",
       "  263,\n",
       "  6555,\n",
       "  1539,\n",
       "  19388,\n",
       "  293,\n",
       "  1072,\n",
       "  9183,\n",
       "  393,\n",
       "  2504,\n",
       "  4769,\n",
       "  3144,\n",
       "  1682,\n",
       "  852,\n",
       "  318,\n",
       "  415,\n",
       "  1296,\n",
       "  322,\n",
       "  9950,\n",
       "  1017,\n",
       "  22193,\n",
       "  19100,\n",
       "  333,\n",
       "  362,\n",
       "  29871,\n",
       "  13,\n",
       "  8000,\n",
       "  297,\n",
       "  6335,\n",
       "  11407,\n",
       "  540,\n",
       "  29886,\n",
       "  2454,\n",
       "  3144,\n",
       "  29884,\n",
       "  535,\n",
       "  29872,\n",
       "  6352,\n",
       "  6656,\n",
       "  29889,\n",
       "  30166,\n",
       "  13,\n",
       "  18502,\n",
       "  898,\n",
       "  967,\n",
       "  330,\n",
       "  368,\n",
       "  19335,\n",
       "  293,\n",
       "  2761,\n",
       "  29892,\n",
       "  4737,\n",
       "  689,\n",
       "  262,\n",
       "  756,\n",
       "  1063,\n",
       "  4318,\n",
       "  304,\n",
       "  11157,\n",
       "  5881,\n",
       "  29875,\n",
       "  586,\n",
       "  6151,\n",
       "  1070,\n",
       "  714,\n",
       "  26807,\n",
       "  29871,\n",
       "  13,\n",
       "  392,\n",
       "  2479,\n",
       "  9418,\n",
       "  29899,\n",
       "  13453,\n",
       "  314,\n",
       "  2922,\n",
       "  706,\n",
       "  4426,\n",
       "  29889,\n",
       "  30166,\n",
       "  13,\n",
       "  3599,\n",
       "  296,\n",
       "  11898,\n",
       "  884,\n",
       "  4368,\n",
       "  7037,\n",
       "  3677,\n",
       "  2185,\n",
       "  2265,\n",
       "  9545,\n",
       "  1549,\n",
       "  297,\n",
       "  6335,\n",
       "  654,\n",
       "  310,\n",
       "  278,\n",
       "  286,\n",
       "  29911,\n",
       "  1955,\n",
       "  29871,\n",
       "  13,\n",
       "  4530,\n",
       "  12818,\n",
       "  2224,\n",
       "  1582,\n",
       "  322,\n",
       "  1462,\n",
       "  23881,\n",
       "  310,\n",
       "  21622,\n",
       "  272,\n",
       "  2614,\n",
       "  29875,\n",
       "  6352,\n",
       "  6656,\n",
       "  29889,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [1,\n",
       "  4737,\n",
       "  689,\n",
       "  262,\n",
       "  338,\n",
       "  697,\n",
       "  310,\n",
       "  278,\n",
       "  1556,\n",
       "  17644,\n",
       "  2225,\n",
       "  23059,\n",
       "  470,\n",
       "  284,\n",
       "  9418,\n",
       "  24947,\n",
       "  16808,\n",
       "  19335,\n",
       "  293,\n",
       "  19518,\n",
       "  29889,\n",
       "  30166,\n",
       "  13,\n",
       "  8011,\n",
       "  7601,\n",
       "  13336,\n",
       "  310,\n",
       "  3158,\n",
       "  20789,\n",
       "  278,\n",
       "  26229,\n",
       "  310,\n",
       "  319,\n",
       "  3580,\n",
       "  29899,\n",
       "  11236,\n",
       "  630,\n",
       "  26823,\n",
       "  19015,\n",
       "  559,\n",
       "  29871,\n",
       "  13,\n",
       "  29898,\n",
       "  19297,\n",
       "  29968,\n",
       "  511,\n",
       "  263,\n",
       "  6555,\n",
       "  1539,\n",
       "  19388,\n",
       "  293,\n",
       "  1072,\n",
       "  9183,\n",
       "  393,\n",
       "  2504,\n",
       "  4769,\n",
       "  3144,\n",
       "  1682,\n",
       "  852,\n",
       "  318,\n",
       "  415,\n",
       "  1296,\n",
       "  322,\n",
       "  9950,\n",
       "  1017,\n",
       "  22193,\n",
       "  19100,\n",
       "  333,\n",
       "  362,\n",
       "  29871,\n",
       "  13,\n",
       "  8000,\n",
       "  297,\n",
       "  6335,\n",
       "  11407,\n",
       "  540,\n",
       "  29886,\n",
       "  2454,\n",
       "  3144,\n",
       "  29884,\n",
       "  535,\n",
       "  29872,\n",
       "  6352,\n",
       "  6656,\n",
       "  29889,\n",
       "  30166,\n",
       "  13,\n",
       "  18502,\n",
       "  898,\n",
       "  967,\n",
       "  330,\n",
       "  368,\n",
       "  19335,\n",
       "  293,\n",
       "  2761,\n",
       "  29892,\n",
       "  4737,\n",
       "  689,\n",
       "  262,\n",
       "  756,\n",
       "  1063,\n",
       "  4318,\n",
       "  304,\n",
       "  11157,\n",
       "  5881,\n",
       "  29875,\n",
       "  586,\n",
       "  6151,\n",
       "  1070,\n",
       "  714,\n",
       "  26807,\n",
       "  29871,\n",
       "  13,\n",
       "  392,\n",
       "  2479,\n",
       "  9418,\n",
       "  29899,\n",
       "  13453,\n",
       "  314,\n",
       "  2922,\n",
       "  706,\n",
       "  4426,\n",
       "  29889,\n",
       "  30166,\n",
       "  13,\n",
       "  3599,\n",
       "  296,\n",
       "  11898,\n",
       "  884,\n",
       "  4368,\n",
       "  7037,\n",
       "  3677,\n",
       "  2185,\n",
       "  2265,\n",
       "  9545,\n",
       "  1549,\n",
       "  297,\n",
       "  6335,\n",
       "  654,\n",
       "  310,\n",
       "  278,\n",
       "  286,\n",
       "  29911,\n",
       "  1955,\n",
       "  29871,\n",
       "  13,\n",
       "  4530,\n",
       "  12818,\n",
       "  2224,\n",
       "  1582,\n",
       "  322,\n",
       "  1462,\n",
       "  23881,\n",
       "  310,\n",
       "  21622,\n",
       "  272,\n",
       "  2614,\n",
       "  29875,\n",
       "  6352,\n",
       "  6656,\n",
       "  29889,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d1594f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c652c67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lora_config=LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=['q_proj','v_proj'],\n",
    "    lora_dropout=0.05,\n",
    "    bias='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dede546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_inst_model_lora = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a8f4f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./tinyllama-lora\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    save_total_limit=1,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d73dee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=non_inst_model_lora,\n",
    "    args=args,\n",
    "    train_dataset=tokenized\n",
    ")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0d9e17e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "c:\\Users\\Roshan Acharya\\Documents\\Roshan\\Projects\\Pytorch\\venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Roshan Acharya\\Documents\\Roshan\\Projects\\Pytorch\\venv\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5, training_loss=9.54721450805664, metrics={'train_runtime': 24.8356, 'train_samples_per_second': 0.805, 'train_steps_per_second': 0.201, 'total_flos': 63629646888960.0, 'train_loss': 9.54721450805664, 'epoch': 5.0})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f56c62de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "# Assuming non_inst_model_lora is your LoRA-wrapped model\n",
    "non_inst_model_lora.save_pretrained(\"./tinyllama-lora\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "899c52d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Ensure pad_token exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load base model in 8-bit for efficiency\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map={\"\": \"cuda\"},  # Start by putting everything on GPU\n",
    "    llm_int8_enable_fp32_cpu_offload=True  # Offload extra parts to CPU in 32-bit\n",
    ")\n",
    "\n",
    "# Load the LoRA adapter you trained\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"./tinyllama-lora\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7847d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-21): 22 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=2048, out_features=256, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=2048, out_features=5632, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=2048, out_features=5632, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=5632, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "lora_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4b2386e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Once upon a time in a faraway land,\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f41fbed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output\n",
    "prompt = \"Clinical trials demonstrated that combining Atorvastatin with Ezetimibe\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8490f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "879ef70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Output:\n",
      "\n",
      "Clinical trials demonstrated that combining Atorvastatin with Ezetimibe significantly improved LDL-C levels and atherosclerosis.\n",
      "Vasopressin receptor antagonists are in development for the prevention of hyperkalemia due to heart failure and congestive heart failure. Vasopressin receptors antagonist have been tested in 2 clinical trials in patients with severe hyperkalemia (>4 mmol/L). Both drugs, vasopressin receptor antagonist (H\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModel Output:\\n\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd024e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
