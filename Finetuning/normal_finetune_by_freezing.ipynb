{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55d1a34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Roshan Acharya\\Documents\\Roshan\\Projects\\Pytorch\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset,load_dataset\n",
    "import os, re\n",
    "\n",
    "# # -------------------------------------------------------------\n",
    "# # 3ï¸âƒ£ Load base model and tokenizer\n",
    "# # -------------------------------------------------------------\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "# # -------------------------------------------------------------\n",
    "# # 4ï¸âƒ£ Freeze everything\n",
    "# # -------------------------------------------------------------\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "# # -------------------------------------------------------------\n",
    "# # 8ï¸âƒ£ Training arguments\n",
    "# # -------------------------------------------------------------\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"  # disable wandb\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./tinyllama-pharma-last4\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     num_train_epochs=2,\n",
    "#     per_device_train_batch_size=2,\n",
    "#     gradient_accumulation_steps=4,\n",
    "#     learning_rate=1e-4,        # slightly higher since fewer params\n",
    "#     fp16=True,\n",
    "#     logging_steps=20,\n",
    "#     save_steps=200,\n",
    "#     save_total_limit=2,\n",
    "#     report_to=\"none\"\n",
    "# )\n",
    "\n",
    "# # -------------------------------------------------------------\n",
    "# # 9ï¸âƒ£ Trainer setup\n",
    "# # -------------------------------------------------------------\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized,\n",
    "#     data_collator=data_collator\n",
    "# )\n",
    "\n",
    "# # -------------------------------------------------------------\n",
    "# # ðŸ”Ÿ Start training\n",
    "# # -------------------------------------------------------------\n",
    "# trainer.train()\n",
    "\n",
    "# # -------------------------------------------------------------\n",
    "# # 11ï¸âƒ£ Save final model\n",
    "# # -------------------------------------------------------------\n",
    "# trainer.save_model(\"./tinyllama-pharma-last4-final\")\n",
    "# print(\"\\nâœ… Training completed and model saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d488614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Trainable: 13.97% of total parameters\n"
     ]
    }
   ],
   "source": [
    "# # -------------------------------------------------------------\n",
    "# # 5ï¸âƒ£ Unfreeze last 4 transformer blocks + lm_head\n",
    "# # -------------------------------------------------------------\n",
    "for name, param in model.named_parameters():\n",
    "    if any(f\"layers.{i}.\" in name for i in range(20, 24)):  # last 4 layers\n",
    "        param.requires_grad = True\n",
    "    if \"lm_head\" in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# # Verify how many parameters are trainable\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"âœ… Trainable: {trainable/total*100:.2f}% of total parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c84dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages extracted with fitz: 1\n"
     ]
    }
   ],
   "source": [
    "#Data prep\n",
    "import fitz\n",
    "pages_text = []\n",
    "def extract_pdf_to_list_fitz(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "   \n",
    "\n",
    "    for page in doc:\n",
    "        text = page.get_text(\"text\").strip()\n",
    "        if text:\n",
    "            pages_text.append(text)\n",
    "    \n",
    "    doc.close()\n",
    "    return pages_text\n",
    "\n",
    "file_path = 'Metformin.pdf'\n",
    "pdf_content_list_fitz = extract_pdf_to_list_fitz(file_path)\n",
    "print(f\"Total pages extracted with fitz: {len(pdf_content_list_fitz)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc6903f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def chunk_paragraphs(txt):\n",
    "\n",
    "    paragraph = []\n",
    "    for block in txt:\n",
    "        chunks=re.split(r'\\n\\s*\\n', block)\n",
    "        for chunk in chunks:\n",
    "            clean=chunk.strip()\n",
    "            if len(clean)>30:\n",
    "                paragraph.append(clean)\n",
    "    return paragraph\n",
    "\n",
    "para=chunk_paragraphs(pages_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c13f88ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[{\"text\":p} for p in para]\n",
    "dataset=Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5fc5b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token=tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09f9f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(examples):\n",
    "    tokens=tokenizer(examples['text'],truncation=True,padding=\"max_length\",max_length=512)\n",
    "    tokens['labels']= [ids.copy() for ids in tokens['input_ids']]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e59274f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 401.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized=dataset.map(tokenize_fn,batched=True,remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81b8b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False   # IMPORTANT: causal LM, not masked LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecde9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"  # disable wandb\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./Models/tinyllama-pharma-last4\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-4,        # slightly higher since fewer params\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a5e01c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "source": [
    "# # 9ï¸âƒ£ Trainer setup\n",
    "# # -------------------------------------------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized,\n",
    "    data_collator=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30417642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:17, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2, training_loss=1.2983741760253906, metrics={'train_runtime': 21.5197, 'train_samples_per_second': 0.372, 'train_steps_per_second': 0.093, 'total_flos': 25424176349184.0, 'train_loss': 1.2983741760253906, 'epoch': 2.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # ðŸ”Ÿ Start training\n",
    "# # -------------------------------------------------------------\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9530c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Training completed and model saved!\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./Models/tinyllama-pharma-last4-final\")\n",
    "print(\"\\nâœ… Training completed and model saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d35bfee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
